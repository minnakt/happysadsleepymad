{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 0.08834883908790225, 'chan': 0.10101957316923549, 'san': 0.12858748206349657, 'always': 0.13259603329859804, 'also': 0.13415129574334472, 'well': 0.13900248540350124, 'work': 0.14484066417239705, 'see': 0.15615725214328813, 'cute': 0.1565663309644919, 'able': 0.1572881495391033, 'yesterday': 0.16324480035129707, 'lot': 0.17199888616337833, 'much': 0.1746345489452752, 'best': 0.1806606783918452, 'become': 0.18321965553397412, 'spring': 0.18493098236378824, 'place': 0.1855256133438672, 'fun': 0.18742188538936355, 'kind': 0.1898573178651859, 'wonderful': 0.19084497788999033, 'time': 0.19343063226680923, 'week': 0.19529528284319098, 'people': 0.19581450266042966, 'coming': 0.19602000307360878, 'still': 0.19651376899794504, 'like': 0.19871209752801722, 'hope': 0.19900455409191886, 'glad': 0.19924133112435904, 'nice': 0.20057979613004318, 'night': 0.2068292324519444, 'tomorrow': 0.2098317509199248, 'seems': 0.21070997230539867, 'okay': 0.21271295565918258, 'please': 0.21284205893500163, 'enjoy': 0.21332332281447008, 'got': 0.21362299661473683, 'even': 0.2144671594834182, 'go': 0.21554143766330291, 'mr': 0.2172562495324324}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# FOR TRANSLATED VERSOIN\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Downloads the NLTK stopword corpus if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# sklearn modules for data processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn modules for LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# sklearn modules for classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# sklearn modules for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "from mtranslate import translate\n",
    "\n",
    "# from translate import Translator\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "import re\n",
    "from many_stop_words import get_stop_words\n",
    "\n",
    "\n",
    "###### Here is tokenizing and filter by stop word.\n",
    "def process_document(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbfðŸ˜Š]','',text)\n",
    "    text = re.sub(r'\\[\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]\\]', r'\\1', text)\n",
    "    \n",
    "    ##### METHOD 1: TRANSLATE FROM BEGINNING #####\n",
    "    text = text.replace(\"ðŸ˜Š\", \"sleepymoji\")\n",
    "    \n",
    "    text = translate(text, 'en')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'(\\w+)')\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    filtered = []\n",
    "    for word in tokenized:\n",
    "        if word not in stop_words:\n",
    "                filtered.append(word)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "###### Here is tokenizing and filter by stop word. here is reading data into a corpus.\n",
    "def read_data(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, errors='ignore', encoding='utf-8') as words_file:\n",
    "        csv_reader = csv.reader(words_file, delimiter = ',')\n",
    "        data = []\n",
    "        for row in csv_reader:\n",
    "            data.append(row[0])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "###### Making the vocabulary.\n",
    "def construct_vocab(corpus):\n",
    "    \"\"\"\n",
    "        Input: A list of list of string. Each string represents a word token.\n",
    "        Output: A tuple of dicts: (vocab, inverse_vocab)\n",
    "                vocab : A dict mapping str -> int. This will be your vocabulary.\n",
    "                inverse_vocab: Inverse mapping int -> str\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    inverse_vocab = {}\n",
    "    id_count = 0\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = id_count\n",
    "                inverse_vocab[id_count] = word\n",
    "                id_count += 1\n",
    "    return (vocab, inverse_vocab)\n",
    "\n",
    "###### Count num words\n",
    "def word_counts(corpus):\n",
    "    \"\"\" Given a corpus (such as returned by load_corpus), return a dictionary\n",
    "        of word frequencies. Maps string token to integer count.\n",
    "    \"\"\"\n",
    "    return collections.Counter(w for s in corpus for w in s)\n",
    "\n",
    "###### Truncate the vocabulary\n",
    "def trunc_vocab(corpus, counts):\n",
    "    \"\"\" Limit the vocabulary to the 10k most-frequent words. Remove rare words from\n",
    "         the original corpus.\n",
    "        Input: A list of list of string. Each string represents a word token.\n",
    "        Output: A tuple (new_corpus, new_counts)\n",
    "                new_corpus: A corpus (list of list of string) with only the 10k most-frequent words\n",
    "                new_counts: Counts of the 10k most-frequent words\n",
    "\n",
    "        Hint: Sort the keys of counts by their values\n",
    "    \"\"\"\n",
    "    new_counts = {}\n",
    "    new_corpus = []\n",
    "    new_counts = collections.Counter(counts).most_common(200)\n",
    "    top_words = set(dict(new_counts).keys())\n",
    "\n",
    "    for sentence in corpus:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word in top_words and word != \"https\" and word != \"amp\" and word != \"co\":\n",
    "                sent.append(word)\n",
    "        new_corpus.append(sent)\n",
    "\n",
    "    new_counts = dict(new_counts)\n",
    "    \n",
    "    \n",
    "    return new_corpus, new_counts\n",
    "\n",
    "###### Constructing word vectors\n",
    "def word_vectors(corpus, vocab):\n",
    "    \"\"\"\n",
    "        Input: A corpus (list of list of string) and a vocab (word-to-id mapping)\n",
    "        Output: A lookup table that maps [word id] -> [word vector]\n",
    "    \"\"\"\n",
    "\n",
    "    # each word vector is [count of word id 1, count of word id 2 ... , count of word id n]\n",
    "    table = {}\n",
    "\n",
    "    # construct a table where every word ID maps to a list of 0's\n",
    "    for word in vocab:\n",
    "        word_id = vocab[word]\n",
    "        table[word_id] = len(vocab) * [0]\n",
    "\n",
    "    for sentence in corpus:\n",
    "        length = len(sentence)\n",
    "        for i in range(length):\n",
    "            curr_word = sentence[i]\n",
    "            for word in range( i - 4, i):\n",
    "                if word >= 0 and word != i:\n",
    "                    table[vocab[curr_word]][vocab[sentence[word]]] += 1\n",
    "            for word in range(i + 1, i + 4 + 1):\n",
    "                if word < length and word != i:\n",
    "                    table[vocab[curr_word]][vocab[sentence[word]]] += 1\n",
    "    return table\n",
    "\n",
    "# iris's function\n",
    "def get_emoji_list(data):\n",
    "    empty = []\n",
    "    for i in range(len(data)):\n",
    "        tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', data[i], flags=re.MULTILINE)\n",
    "        a = process_document(tweet)\n",
    "        a = ' '.join(a)\n",
    "        data[i] = a\n",
    "        if a == '':\n",
    "            empty.append(i)\n",
    "    data  = list(np.delete(list(data), empty, 0))\n",
    "\n",
    "###### Return most similar words\n",
    "def most_similar(lookup_table, wordvec, vocab):\n",
    "    \"\"\" Helper function (optional).\n",
    "\n",
    "        Given a lookup table and word vector, find the top most-similar word ids to the given\n",
    "        word vector. You can limit this to the first NUM_CLOSEST results.\n",
    "    \"\"\"\n",
    "\n",
    "    most_similar = {}\n",
    "    for word in lookup_table:\n",
    "        vector = lookup_table[word]\n",
    "        pair = []\n",
    "        pair.append(vector)\n",
    "        pair.append(wordvec)\n",
    "        distance = pdist(pair, 'cosine')\n",
    "        most_similar[word] = distance\n",
    "\n",
    "    sorted_most_similar = sorted(most_similar.items(), key=lambda x: x[1])\n",
    "    sorted_most_similar = sorted_most_similar[1:100 + 1]\n",
    "    most_similar_word = []\n",
    "    for word in sorted_most_similar:\n",
    "        most_similar_word.append(word[0])\n",
    "\n",
    "    return most_similar_word\n",
    "\n",
    "def get_wordvec_dictionary(lookup_table, wordvec, inverse_vocab):\n",
    "    \"\"\" Helper function (optional).\n",
    "\n",
    "        Given a lookup table and word vector, \n",
    "        returns a dictionary of the words and their distance from the given word\n",
    "    \"\"\"\n",
    "\n",
    "    most_similar = {}\n",
    "    for word in lookup_table:\n",
    "        vector = lookup_table[word]\n",
    "        pair = []\n",
    "        pair.append(vector)\n",
    "        pair.append(wordvec)\n",
    "        distance = pdist(pair, 'cosine')\n",
    "        most_similar[word] = distance\n",
    "\n",
    "    sorted_most_similar = sorted(most_similar.items(), key=lambda x: x[1])\n",
    "    sorted_most_similar = sorted_most_similar[1:40]\n",
    "    \n",
    "    word_distance = {}\n",
    "    for word in sorted_most_similar:\n",
    "        num = word[1][0]\n",
    "        word_distance[inverse_vocab[word[0]]] = num\n",
    "    return word_distance\n",
    "\n",
    "###### TSNE plot\n",
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "        \n",
    "        \n",
    "def main(data_dir):\n",
    "\n",
    "    corpus = read_data(data_dir) # returns the data, one tweet per list\n",
    "    # call  translate here if necessary\n",
    "    new_corpus = []\n",
    "    for tweet in corpus:\n",
    "        new_corpus.append(process_document(tweet)) # returns a list of tweets processed\n",
    "    \n",
    "    corpus_2d = [] # list of lists\n",
    "    for tweet in new_corpus:\n",
    "        corpus_2d.append(tweet)\n",
    "    \n",
    "#     print(corpus_2d)\n",
    "    counts = word_counts(corpus_2d)\n",
    "    new_corpus, new_counts = trunc_vocab(corpus_2d, counts)\n",
    "               \n",
    "    vocab, inverse_vocab = construct_vocab(new_corpus)\n",
    "\n",
    "    lookup_table= word_vectors(new_corpus, vocab)\n",
    "    \n",
    "    vectors = []\n",
    "    for wid in lookup_table:\n",
    "        vectors.append(lookup_table[wid])\n",
    "\n",
    "    D = pdist(vectors, 'cosine')\n",
    "    D = squareform(D)\n",
    "    \n",
    "    dictionary_data = get_wordvec_dictionary(lookup_table, \n",
    "    lookup_table[vocab['sleepymoji']], inverse_vocab)\n",
    "\n",
    "    print(dictionary_data)\n",
    "\n",
    "    with open('for_graphs/smiling_ja.json', 'wt') as out:\n",
    "        res = json.dump(dictionary_data, out, indent=4, separators=(',', ': '))\n",
    "    \n",
    "#     tsne = TSNE(\n",
    "#       perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "#     plot_only = 12\n",
    "#     low_dim_embs = tsne.fit_transform(D[:plot_only, :])\n",
    "#     labels = [inverse_vocab[i] for i in range(plot_only)]\n",
    "#     plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "\n",
    "# This may take a little bit of time (~30-60 seconds) to run.\n",
    "if __name__ == '__main__':\n",
    "    data_dir = 'data/smiling_face_ja.csv'\n",
    "    main(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
